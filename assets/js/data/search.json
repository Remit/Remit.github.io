[ { "title": "Designing an Enterprise System (Part II)", "url": "/posts/systems-part-2/", "categories": "Software Design", "tags": "enterprise software, software, software architecture, modularization, self-adaptability, configurability", "date": "2023-01-25 10:00:00 +0100", "snippet": "Designing an Enterprise System, Part II: Aligning with the Use Case via System DesignBalancing between Specialization and GeneralizationIn the second part of the blog post on designing an enterprise system, I will focus on the high-level technical aspects that the software should embody to align with its use cases. This blogpost does not aim to provide an exhaustive list of things you should do to align your software with the use case expectations. Instead, the goal here is to highlight the general connection between the expectations of the use cases and how they can be utilized to shape the software system. You can read more about these expectations in the previous part.The previous blog post depicted the alignment of the software with the use case as the most important category of expectations to fulfill. The daunting issue of fulfilling these kinds of expectations is to design software that both aligns with every use case and remains generic enough to be useful beyond one specific application.If the software aligns with a very narrow set of requirements of a particular use case at the expense of other use cases, then we enter the territory of custom software development. This is a kind of service that IT consultancy companies commonly offer to their customers. Custom software is required when the use case in question is unique. For instance, the use cases in the public administration domain fall well into this category since they need to align with how a specific society is structured in terms of institutions, laws, economic relations, and even customs.On the other end, we have some very generic software systems that have a very uniform value proposition across a very wide spectrum of uses. Operating systems are one of the prime examples. In a nutshell, an operating system provides the same basic functionality of hardware resource management to all of its users. It does not attempt to differentiate between its users. Sure, a user can customize their operating system of choice in some way, but the out-of-the-box value proposition is exactly the same for every user.Between the custom software and general-purpose systems, there is a whole range of software systems of various degrees of specialization. So, what are the ways to achieve better alignment with the use cases and at the same time to cover a lot of ground?Alignment PracticesFunctional Modularization to Cover the Diverse Use CasesFunctional modularization is one of the most common ways to generalize the software functionality in order to expand the portfolio of the supported use cases and at the same time to keep its value differentiated for every single one of them. Unlike what the name of this approach suggests, the first step is actually to figure out the common denominator, that is the functionality shared across the use cases. For instance, the data movement service and adherence to some standard protocol for seamless integration are the shared requirements for the use cases in the Internet of Things domain. Once we zoom in, we discover that the specific requirements of every use case are quite unique in comparison to the use cases in other industries. That’s where functional modularization comes to rescue.Once the common functionality has been distilled from the multitude of the use cases and embodied in the solution, it is the time to discover the finer groupings of the use cases. The use cases should be grouped based on similarity of their requirements to the functionality. When grouping is done properly, the functionality required by a group of use cases will likely not intersect with the functionality required by another group of the use cases. This is what modularization helps us with. The discovered niche functionality is wrapped into a software package (module) that can be plugged into the core software system. Sometimes these packages are called extensions, since they extend the core offering of the system in certain ways.To expand on the data movement example, one can imagine a use case in industrial automation that demands swift decisions to adapt the flow of parts on the manufacturing line if the parts start to accumulate somewhere along the line. Acceptable latency of at most several milliseconds and constrained capabilities of the shop floor hardware may render the deployment of a full-blown analytics system unfeasible. However, if the data movement platform supports extensions, then fulfilling the strict latency and resource utilization requirements can be achieved by plugging a custom analytics module directly into the data movement software running on the edge. Such an extension will not offer the rich capabilities of a dedicated analytics platform deployed on a rack full of servers, but will do its job just as well. By adding an extension to the data movement service, one gets the alignment with the strict requirements of the use case. This is achieved by extension staying close to where the data is and utilizing the existing edge infrastructure.Designing for Nonfunctional RequirementsIf the gaps between the needed functionality across niches are easy to address with help of modularization, the requirements to the characteristics of how these functions are performed (e.g. how fast, how reliably, etc) are not that easy to compartmentalize. It turns out that such requirements often touch the very core of the enterprise software and are quite frequently at odds with each other. For instance, if the use case requires the data to be stored extremely reliably and consistently, then the network partition may propel the system to pause the queries to ensure that its state converges after the partition is healed. On the other end, another use case may care less about whether the retrieved data is fully consistent at any given point in time, but requires the operations to be performed at the very instant when they land on the software. By increasing the ability of the software to meet the requirements to data reliability and convergence for one niche, the overall availability of the software may fail to meet the expectations of other use cases. Such tradeoffs are typical in engineering of complex enterprise systems, especially once they become distributed. There are multiple high-level approaches that can be taken to address these tensions.Compromise Solution:First of all, one can think of a specific implementation as being one of the possibilities in a multidimensional design space. The axes in this space represent the characteristics of the system that are relevant to the use cases, e.g. latency, resource usage. One studies this multidimensional space to find a point that meets the expectations of how the functions of the software are performed for the majority of the use cases. The advantage of finding such a compromise solution is that the implementation does not get a lot of moving parts and does not become unnecessarily sophisticated to maintain. The downside of hitting a compromise is that the use cases with very strict requirements on the functionality delivered by the software, like sub-millisecond message passing latency, may find it impossible to rely on such a compromise solution. In addition to that, finding the right balance of characteristics that the software exhibits requires substantial engineering efforts and may likely result in the development slowdown. Largely, the quality of the compromise solution depends on diversity of the use cases available to the solution creators.Configurability:Another option is to make software configurable to align better with the specific demands of the use case. By setting some flags and changing the numerical options, the use case owner may adapt the software to use less memory in exchange for certain functions being performed slower. Such configurable tradeoffs are frequently a good alternative to the rigid design resulting from a compromise since the use cases are likely to rely only on a subset of characteristics of how the functions are performed by the software. This path is often taken by the providers of the so-called platform solutions that should be adapted to the use cases running on top of them.Kubernetes is one of such examples. Kubernetes offers a lot of configurations to adapt itself to the requirements of an incredibly diverse set of applications. Naturally, its rich configurability led to a complexity explosion. As is the case with any solution going down this path, Kubernetes is frequently criticized with an abundance of moving parts and configurations that might be quite complicated to set up for the use case if the use case owner lacks deep understanding of the platform.The workaround for the configuration complexity explosion is usually in providing the consultation services either by establishing a dedicated team within the company or by referring for help to the third-party consultancies. Although this is a widely-adopted practice, still, this leads to frustration in using the solution as well as to monetary and time overhead. Last but not least, this ‘hotfix’ makes use case owners even less self-reliant.Self-adaptability:To address the complexity explosion resulting from parameterization of the system configuration, engineers sometimes introduce self-adaptive components to the system. The main purpose of making the system or the parts thereof self-adaptive is to avoid manual tuning of software to every specific use case. Every self-adaptive component manages the software behavior in some way. Very commonly, self-adaptive components in the modern systems are introduced to handle the resource management aspect. Overload protection and autoscaling mechanisms are the most prominent representatives of this self-adaptation subdomain.Overload protection manages the incoming load. If the current or anticipated shortage of system resources does not (will not) allow it to process the incoming load, then the incoming load might get throttled or redirected to some other deployment or a running instance. Thus, overload protection deals with sizing the incoming load to adjust it to the available resources. Its counterpart in cloud environments is autoscaling.The purpose of autoscaling is to dynamically adapt the resources to the incoming load. The decision to add or remove the instances of software (horizontal scaling) or to allocate more or less resources to the running instance(s) of the software (vertical scaling) is taken automatically, based on the available indicators, e.g. current or forecasted resource utilization. Autoscaling can (and should!) be combined with the overload protection. If the former can handle most of the changes in workload, the unanticipated explosive traffic growth can be handled by overload protection until the autoscaling mechanism catches up. This kind of traffic is known as flash crowd.Although I refer to such mechanisms as autoscaling and overload protection as ‘self-adaptive’, self-adaptation is frequently a subject to some form of tuning, so it is not entirely ‘self-adaptive’. The tuning opportunities usually include selection of the managed parameters, such as instance count or the amount of resources allocated to the running Kubernetes pods, and of observed indicators used to decide if an adaptation action should be performed. Regardless of the specific tuning, the adaptation mechanism itself can rarely be changed.Despite the idea of self-adaptive components being very attractive in a sense of removing the workload-dependent assumptions from the software design process, the obstacles to introducing more of such components into the enterprise software are profound. First of all, it is not always clear what impact would the interplay between self-adaptation of multiple system characteristics yield for the system as a whole. It may even happen that poorly-tuned self-adaptation mechanisms will behave worse than the predefined configurations, e.g. too aggressive overload protection may render the system unavailable despite having sufficient resources. Secondly, the design of self-adaptation mechanisms often demands very deep insights into the system behavior and the workloads. Unfortunately, such insights are not usually available to the designers of the software. Specifically, workloads and the infrastructure that the software runs on may differ a lot depending on the specific use case and the budgets available for the use case owner. Yet another reason is rooted in the limited transparency of the self-adaptation mechanisms. One can easily identify the root cause of decisions taken by such mechanisms if they are based on formulas combining well-understood parameters such as memory usage or the latency of requests in the system. However, the use of more abstract primitives like credits in the overload protection lends itself to misinterpretation.The listed challenges in no way mean that it is futile to introduce the self-adaptive mechanisms into the systems. What I intend to convey is that such mechanisms demand careful design and evaluation of their possible interactions when added to the software.Key TakeawaysIn this post, I’ve tried to summarize the high-level software design strategies that help to align it with multiple use cases but at the same time not to dive into the custom software development.So, here they are…To align on the functional axis: functional modularization - core software product that meets the shared requirements of the use cases whose functionality can be expanded with modules.To align on how the functions are performed (non-functional requirements): discover a compromise solution - software should be adapted to meet the demands of the majority of the use cases, the extremes can be ignored; configurability - how the software performs its functions can be configured; self-adaptability - parts of (or the whole) software product adapts itself to the requirements of a specific use case dynamically.The above strategies can also be mixed to get the best out of all the available options.Share your experience!Which of the above strategies did you find successful and which - not? What other software design strategies do you follow? Feel free to share your insights in the comments or in the private messages." }, { "title": "Designing an Enterprise System (Part I)", "url": "/posts/systems-part-1/", "categories": "Software Design", "tags": "customer expectations, enterprise software, software, requirements analysis", "date": "2023-01-15 13:00:00 +0100", "snippet": "Designing an Enterprise System, Part I: Customer ExpectationsHow Enterprise Systems Emerge and What Contributes to ThisWell-entrenched in their businesses for decades, enterprises grow complex processes that they eventually start to automate and optimize. These processes emerge in different domains, from business value production all the way to the distribution and supporting functions, like hiring.Both automation and optimization of the business processes are reachable with means of the software. However, if automation is completely unachievable without the software, business process optimization does not necessarily entail its addition into the process. Business process optimization predominantly relies on domain experts rethinking the existing processes and adapting them to produce more revenue or to reduce costs of following them. Formalizable domain expertise is a rich soil for introducing the software into the business processes.Regardless of the exact purpose that the software is used for, enterprises can always choose between building (and maintaining!) an in-house solution or purchasing one from a third party. The latter naturally benefits from the resource pooling effect. The idea behind this effect is rather straightforward: in short, third-party solution providers, such as product companies, tackle a niche need that is present across a spectrum of the use cases. For instance, there is a need to move the data from some sort of producers, like sensors, to the consumers, like analytical applications. This is quite a common need for businesses in many industries. Multiple use case owners, like businesses or business units, utilize the same third-party solution to fulfill their common need. This enables the provider of this solution to pool the incoming financial and other kinds of resources for the sake of focusing its efforts on improving the solution.Why is resource pooling needed? When taken separately, the available resources of each use case owner, be they financial or experience, would be insufficient to create the solution of the desired quality and to maintain it. The insufficiency is often due to this need being only a small piece of the complex core business puzzle. For example, car manufacturers are in the business of producing and selling cars. Although data movement is an inherent part of their factory automation processes or connected vehicles technology platform, this might be a mere tiny link of a very long and versatile value-adding chain. Thus, it might not be economically justifiable to make the company completely self-reliant in respect to this small piece. However, when pooled, financial, know-how, as well as the other resources, become sufficient to produce the solution that fulfills the need in the best possible way.When opting for the third-party solution providers, the use case owners have a multitude of expectations. These expectations offer themselves for a neat grouping into a few categories. When discussing these categories, I’ll focus on what I believe to be the most important expectations that translate into how the solution is shaped from the technological point of view. Every expectation is formulated as a requirement to help you have an easier time finding examples and counterexamples in your experience.Expectations CategoriesCategory I: Alignment with the Use CaseThis category is the most important one, and the reason for that is straightforward. If the solution is not well-aligned with the use case, then the business of the third-party solution provider will face huge impediments to its growth due to having hard time convincing potential customers to rely on the offering. On a high level, this is where both the value proposition and the positioning of the solution play a crucial role.Expectation 1. Mapping from the primitives and the patterns of the use case to the primitives and the patterns of the solution should be clear.This expectation means that the software system should come as close as possible to using the primitives of the business process that it aims to automate or optimize.If we consider an MQTT broker implementing a publish-subscribe messaging pattern, then we may see how well its message distribution pattern captures the data flow from the single source of truth (sensors) to various services that each give different arrangement and view on the same data. These varying views on the same data are required to fulfill different business needs relying on the same data, such as parts ordering, production optimization, cost management, etc. Intuitively, complimentary services relying on the same data (or the data produced by the same entities such as manufacturing lines) push the use case owner to think in a paradigm of data distribution. As an example, all the measurements from a sensor are required to dynamically adjust the production process, but just a single outlier measurement could suffice to alarm the personnel that they should leave the shop floor.Expectation 2. (Non-functional) characteristics of functions performed by the software should align with the expectations from the business process that the software is used in.Depending on the use case, the characteristics may differ or have different priorities. If the continuity of the business process depends on the software, then it has to be reliable (avoiding or minimizing the data loss) and it has to be at least as available as the other systems/components interact with it. In other use cases, the delay introduced by the software is expected to be within some bounds. For example, the cumulative time budget allocated to the website backend processing the user request, such as adding an item to the cart, can reach a hundred milliseconds. On the other hand, adaptive production lines deal with physical objects rather than with human perception, hence the expectation might be that the delay is at most a few milliseconds.Alignment demands third-party solution providers to have really deep insights into the business cases of their customers. For the general-purpose solutions used in business processes across diverse use cases, such as ones loosely unified under an umbrella term ‘Internet of Things’, this might be extremely tricky since the expectations might be contradictory. For instance, by attempting to increase the reliability of the software system in face of failures, it might be unwillingly made slower since more time is needed to reconcile the updates performed on the data. Or, by decreasing the end-to-end latency of the software system, its memory usage may grow substantially since more results are now cached on the application level. If the expectations from the software in different use cases diverge too widely, then it might be the right time to introduce a new specialized product, should the opportunity be attractive enough.Category II: Frictionless operating experienceOne might of course argue against this category since the value proposition of the software might outweigh its rough edges. However, when the market is competitive, providing frictionless experience becomes crucial. Multiple expectations contribute to this category, but the key idea is that anyone using the software in their business processes wants it to run smoothly. In the end, not many of us like to be reminded that we or the world around us is far from perfect.Expectation 3. The new software should run on the existing infrastructure and require little to no training.Although this might be an impediment when we are talking about tech enthusiasts, in general, people would be willing to spend as little effort as possible to integrate software into their business processes and operate it. Every reference to the manual or the support service creates friction. Every such event conveys the message that the person operating the software cannot do it on their own, which inevitably leads to negative emotions. This is, of course, the least desirable outcome for the product.Expectation 4. The new software should incur as little additional costs as possible.This expectation is shaped by the costs already being specified in the contract. However, even if the software license was already paid for, the required infrastructure, maintenance, and operating costs may fall out of the contract’s scope. To a large extent, SaaS solutions are exempt from this issue. Nevertheless, not all the use cases align well with this distribution format, so offering a SaaS-only solution might become a limiting factor rather than an enabling one, depending on the use cases in question. If the software needs to be operated by the use case owner (perhaps, due to need to stay available even if the network connection is lost), its appetite towards compute and storage resources might surface as an additional cost on top of what one already pays for the software. Design choices made in software impact the usage of resources too. In case of a SaaS solution, these costs are being paid by the solution provider, hence the incentive to reduce them is high. In contrast, the on-premise solution’s appetites are being paid for by the use case owner. In the latter case, there is a high chance of friction emerging since the feedback mechanism is poorly (if at all) defined: the impact of high resource usage is not experienced by the solution provider first-hand.Expectation 5. The new software should seamlessly integrate into the business process.It should be expected that the use cases are to some extent (or even end-to-end) automated. In this case, the software solution offered by the third party provider must align well with the existing systems and avoid creating hiccups when in operation. Technically, this boils down to clear definition of APIs and correct usage of APIs of other software in operation. Even if this sounds easy, there is an additional twist: API versioning. Deprecation of the old API calls and the introduction of the new ones could impair the added software’s capability to perform expected tasks. In addition to offering the required functionality, the added piece of software should not become a bottleneck in the business process, meaning that it should integrate with other systems not only on a functional level but also on a level of its quality characteristics, like introduced delay, resource usage, and throughput.Expectation 6. The new software should be future-proof (since the business conditions change).It is to be expected that the use case relying on the third-party software gets new requirements, such as keeping the data secure or tracing the requests through the chain of software systems. In that case, the added software needs to enable integrations for the new systems that it potentially does not even have an API for! Since there is no way to foretell all the possible integrations that the software system might need to support in the future, the best that one can hope for is either to make it easily extendable or to constantly expand its functionality to adapt to the changing requirements of the use cases. If the resources of the solution provider are scarce and the number of the use cases grows explosively, then it makes sense to introduce extensibility in the form of SDKs and pluggable modules. Should the system be applied in a new use case, the use case owner can swiftly adapt the system to their needs by creating a custom module.Category III: Technological transparencyThis whole category is obliged for its existence to the very fear of the unknown that lives within every human being. Naturally, this fear is directed at the technologies as well. Both due to the intellectual property protection reasons and the high complexity of the modern software systems, third-party solution providers are incentivized to hide as much as possible about the inner workings of their software. In turn, this leads to the use case owners not being able to fully rely on the software. They then can opt to continue using the cumbersome non-automated way of doing things since every action is known and has been tried multiple times before. To get a foot in the door, the third-party solution provider needs to ensure that its offering is transparent to the extent that it helps the user rely on it. If the underlying technology is very complex to explain, then it is even more the case that the time and effort should be taken to provide an understandable explanation of how the software behaves and why it does what it does.Expectation 7. The new software should be based on proven technologies.If the software solution uses some technology that no other software with the same or similar functionality uses, then the use case owners might express uncertainties about the implications of using the technology that they are not accustomed to. This issue gets particularly noticeable in the absence of significant advantages over other comparable solutions. The logic here is quite simple - any unknown technology is an additional risk. If a new risk factor is introduced to the use case, then it should offer an order of magnitude improvement over the competing solutions for at least one measurement of importance.In addition to the risk-aversion that every one of us has to some degree, the expectation for the software to be based on proven technologies has a rational explanation as well. The behavior of the so-called ‘proven’ technologies is quite well-known thanks to them having large user bases. The more users, the more use cases were thrown at the technologies, thus most of the edge cases have already been discovered and accounted for. Discovering such edge cases comes hand-in-hand with documenting them, hence ‘proven’ technologies are also proven in a way that they have a larger body of knowledge explaining peculiarities of their behaviors and configurations.Conclusion and OutlookSometimes directly, sometimes not, the outlined expectations determine the success of the third-party software solutions. When designing and implementing a solution, one needs to keep them in mind. Even though these expectations might seem to be not actionable, the ability to meet them determines the long-term success of the product. Therefore, when building a software product, one will profit by passing it through a filter of these expectations from time to time.Although the presented list is in no way exhaustive, it is a starting point for the discussion on how the expectations translate into the technical requirements to the functionality and characteristics of how the functions are performed.Tell me what is your take on the expectations from the third-party solutions! If you want me to dive into a related topic, feel free to reach out or leave a comment." }, { "title": "Purpose of this blog", "url": "/posts/my-first-blogpost/", "categories": "Blogging", "tags": "writing", "date": "2022-01-02 13:40:00 +0100", "snippet": "Working part-time as a journalist at the university newspaper for a couple of years and publishing research papers for at least 11 years, I became aware that writing stuff down makes one’s thinking process clearer.This is why I practice writing as my daily routine.It helps me both to set the goals for the coming day and to shape my understanding of the code when chasing some bizarre bug.Writing helps to capture important ideas from the books that I read and to fit them into my own mental model using the right words and associations.David Deutsch contributed a beautiful and deep idea to the body of modern philosophy of science.He views progress as an ongoing process of improving explanations.Humanity proceeds from one explanation of some phenomenon or an event to another, better one.Improving our explanations of some events, other people behaviours, or even technologies result in better understanding and a holistic outlook on everything that surrounds us.Writing helps us on this path.Writing explanation after explanation sheds more light onto the phenomena that we are interested in.By gradually improving the written explanation, one can acquire better picture of the subject in focus.If we apply the approach of improving written explanations to technologies and tools, over time, we will get a refined understanding of inner workings of particular technology or a tool and how it best fits to solving a particular problem.More importantly, we will also identify the limitations of the technology and the trajectory of the technology’s future development.With this blog, I want both to improve my own understanding of technologies that I come into contact with and to share my insights with those few that are ready to dive into the tech." } ]
